{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "import argparse\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    df = pd.read_csv('aapl_msi_sbux.csv')\n",
    "    return df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The experience replay memory ###\n",
    "class ReplayBuffer:\n",
    "    def __init__(self,obs_dim,act_dim, size):\n",
    "        self.obs1_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
    "        self.obs2_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
    "        self.acts_buf = np.zeros(size, dtype = np.unit8)\n",
    "        self.rews_buf = np.zeros(size, dtype = np.float32)\n",
    "        self.done_buf = np.zeros(size, dtype = np.unit8)\n",
    "        self.ptr, self.size, self.max_size = 0,0, size\n",
    "    \n",
    "    def store(self,obs, act, rew, next_obs, done):\n",
    "        self.obs1_buf[self.ptr] = obs\n",
    "        self.obs2_buf[self.ptr] = next_obs\n",
    "        self.axts_buf[self.ptr] = act\n",
    "        self.rews_buf[self.ptr] = rew\n",
    "        self.done_buf[self.ptr] = done\n",
    "        self.ptr = (self.ptr+1) % self.max_size\n",
    "        self.size = min(self.size+1, self.max_size)\n",
    "        \n",
    "    def sample_batch(self, batch_size=32):\n",
    "        idxs = np.random.randint(0, self.size, size = batch_size)\n",
    "        return dict(s = self.obs1_buf[idxs],\n",
    "                   s2 = self.obs2_buf[idxs],\n",
    "                   a = self.acts_buf[idxs],\n",
    "                   r = self.rews_buf[idxs],\n",
    "                   d = self.done_buf[idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scaler(env):\n",
    "    #return scikit-learn scaler object to scale the states\n",
    "    \n",
    "    states =[]\n",
    "    for _ in range(env.n_step):\n",
    "        action = np.random.choice(env.action_space)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        states.append(state)\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(states)\n",
    "    return scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_make_dir(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(input_dim, n_action, n_hidden_layers=1, hidden_dim=32):\n",
    "    #Multi Layer Perceptron\n",
    "    \n",
    "    #Input Layer\n",
    "    i = Input(shape=(input_dim,))\n",
    "    x=i\n",
    "    \n",
    "    #Hidden layers\n",
    "    for _ in range(n_hidden_layers):\n",
    "        x = Dense(hidden_dim,activation='relu')(x)\n",
    "        \n",
    "    #Final layer\n",
    "    x = Dense(n_action)(x)\n",
    "    \n",
    "    #Make model\n",
    "    model = Model(i,x)\n",
    "    \n",
    "    model.compile(loss='mse', optimizer= 'adam')\n",
    "    print((model.summary()))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiStockEnv:\n",
    "    '''\n",
    "    A 3-stock trading environment.\n",
    "    State: vector size 7 (nstock*2+1)\n",
    "     # Share stock 1\n",
    "     # Share stock 2\n",
    "     # Share stock 3\n",
    "     Price stock 1\n",
    "     Price stock 2\n",
    "     Price stock 3\n",
    "     Cash owned\n",
    "     Action: categorical variable with 27 (3^3) possibilites\n",
    "     for each stock you can:\n",
    "     0 = sell\n",
    "     1 = hold\n",
    "     2 = buy\n",
    "     '''\n",
    "    \n",
    "    def __init__(self, data, initial_investment = 20000):\n",
    "        #data\n",
    "        self.stock_price_history = data\n",
    "        self.n_step, self.n_stock = self.stock_price_history.shape\n",
    "        \n",
    "        # instance attributes\n",
    "        self.initial_investment = initial_investment\n",
    "        self.cur_step = None\n",
    "        self.stock_owned = None\n",
    "        self.stock_price = None\n",
    "        self.cash_in_hand = None\n",
    "        \n",
    "        self.action_space = np.arange(3**self.n_stock)\n",
    "        \n",
    "        #action permutations returns a nested list with elements\n",
    "        \n",
    "        self.action_list = list(map(list, itertools.product([0,1,2], repeat = self.n_stock)))\n",
    "        \n",
    "        #calc size of state\n",
    "        self.state_dim = self.n_stock * 2 + 1\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    \n",
    "    def reset(self):\n",
    "        self.cur_step = 0\n",
    "        self.stock_owned = np.zeros(self.n_stock)\n",
    "        self.stock_price = self.stock_price_history[self.cur_step]\n",
    "        self.cash_in_hand = self.initial_investment\n",
    "        return self._get_obs()\n",
    "    \n",
    "    def step(self, action):\n",
    "        assert action in self.action_space\n",
    "        \n",
    "        #get current value before performing action\n",
    "        prev_val = self._get_val()\n",
    "        \n",
    "        #update price - next day\n",
    "        self.cur_step +=1\n",
    "        self.stock_price = self.stock_price_history[self.cur_step]\n",
    "        \n",
    "        #perform trade\n",
    "        self._trade(action)\n",
    "        \n",
    "        #get new value after taking action\n",
    "        cur_val = self._get_val()\n",
    "        \n",
    "        #reward is the increase in portfolio value\n",
    "        reward = cur_val - prev_val\n",
    "        \n",
    "        #done if we run out of data\n",
    "        done = self.cur_step == self.n_step - 1\n",
    "        \n",
    "        #store the current value of portfolio\n",
    "        info = {'cur_val': cur_val}\n",
    "        \n",
    "        #conform to Gym API\n",
    "        return self._get_obs(), reward, done, info\n",
    "    \n",
    "    \n",
    "    def _get_obs(self):\n",
    "        obs = np.empty(self.state_dim)\n",
    "        obs[:self.n_stock] = self.stock_owned\n",
    "        obs[self.n_stock:2*self.n_stock] = self.stock_price\n",
    "        obs[-1] = self.cash_in_hand\n",
    "        return obs\n",
    "    \n",
    "    def _get_val(self):\n",
    "        return self.stock_owned.dot(self.stock_price) + self.cash_in_hand\n",
    "    \n",
    "    def _trade(self, action):\n",
    "        action_vec = self.action_list[action]\n",
    "        \n",
    "        sell_index = []\n",
    "        buy_index = []\n",
    "        for i, a in enumerate(action_vec):\n",
    "            if a == 0:\n",
    "                sell_index.append(i)\n",
    "            elif a == 2:\n",
    "                buy_index.append(i)\n",
    "                \n",
    "        if sell_index:\n",
    "            for i in sell_index:\n",
    "                self.cash_in_hand += self.stock_price * self.stock_owned[i]\n",
    "                self.stock_owned[i] = 0\n",
    "        \n",
    "        if buy_index:\n",
    "            can_buy = True\n",
    "            while can_buy:\n",
    "                for i in buy_index:\n",
    "                    if self.cash_in_and > self.stock_price[i]:\n",
    "                        self.stock_owned[i] += 1\n",
    "                        self.cash_in_hand -= self.stock_price[i]\n",
    "                    else:\n",
    "                        can_buy = False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(object):\n",
    "  def __init__(self, state_size, action_size):\n",
    "    self.state_size = state_size\n",
    "    self.action_size = action_size\n",
    "    self.memory = ReplayBuffer(state_size, action_size, size=500)\n",
    "    self.gamma = 0.95  # discount rate\n",
    "    self.epsilon = 1.0  # exploration rate\n",
    "    self.epsilon_min = 0.01\n",
    "    self.epsilon_decay = 0.995\n",
    "    self.model = mlp(state_size, action_size)\n",
    "\n",
    "\n",
    "  def update_replay_memory(self, state, action, reward, next_state, done):\n",
    "    self.memory.store(state, action, reward, next_state, done)\n",
    "\n",
    "\n",
    "  def act(self, state):\n",
    "    if np.random.rand() <= self.epsilon:\n",
    "      return np.random.choice(self.action_size)\n",
    "    act_values = self.model.predict(state)\n",
    "    return np.argmax(act_values[0])  # returns action\n",
    "\n",
    "\n",
    "  def replay(self, batch_size=32):\n",
    "    # first check if replay buffer contains enough data\n",
    "    if self.memory.size < batch_size:\n",
    "      return\n",
    "\n",
    "    # sample a batch of data from the replay memory\n",
    "    minibatch = self.memory.sample_batch(batch_size)\n",
    "    states = minibatch['s']\n",
    "    actions = minibatch['a']\n",
    "    rewards = minibatch['r']\n",
    "    next_states = minibatch['s2']\n",
    "    done = minibatch['d']\n",
    "\n",
    "    # Calculate the tentative target: Q(s',a)\n",
    "    target = rewards + (1 - done) * self.gamma * np.amax(self.model.predict(next_states), axis=1)\n",
    "\n",
    "    # With the Keras API, the target (usually) must have the same\n",
    "    # shape as the predictions.\n",
    "    # However, we only need to update the network for the actions\n",
    "    # which were actually taken.\n",
    "    # We can accomplish this by setting the target to be equal to\n",
    "    # the prediction for all values.\n",
    "    # Then, only change the targets for the actions taken.\n",
    "    # Q(s,a)\n",
    "    target_full = self.model.predict(states)\n",
    "    target_full[np.arange(batch_size), actions] = target\n",
    "\n",
    "    # Run one training step\n",
    "    self.model.train_on_batch(states, target_full)\n",
    "\n",
    "    if self.epsilon > self.epsilon_min:\n",
    "      self.epsilon *= self.epsilon_decay\n",
    "\n",
    "\n",
    "  def load(self, name):\n",
    "    self.model.load_weights(name)\n",
    "\n",
    "\n",
    "  def save(self, name):\n",
    "    self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] -m MODE\n",
      "ipykernel_launcher.py: error: the following arguments are required: -m/--mode\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "def play_one_episode(agent, env, is_train):\n",
    "  # note: after transforming states are already 1xD\n",
    "  state = env.reset()\n",
    "  state = scaler.transform([state])\n",
    "  done = False\n",
    "\n",
    "  while not done:\n",
    "    action = agent.act(state)\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    next_state = scaler.transform([next_state])\n",
    "    if is_train == 'train':\n",
    "      agent.update_replay_memory(state, action, reward, next_state, done)\n",
    "      agent.replay(batch_size)\n",
    "    state = next_state\n",
    "\n",
    "  return info['cur_val']\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "  # config\n",
    "  models_folder = 'rl_trader_models'\n",
    "  rewards_folder = 'rl_trader_rewards'\n",
    "  num_episodes = 2000\n",
    "  batch_size = 32\n",
    "  initial_investment = 20000\n",
    "\n",
    "\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument('-m', '--mode', type=str, required=True,\n",
    "                      help='either \"train\" or \"test\"')\n",
    "  args = parser.parse_args()\n",
    "\n",
    "  maybe_make_dir(models_folder)\n",
    "  maybe_make_dir(rewards_folder)\n",
    "\n",
    "  data = get_data()\n",
    "  n_timesteps, n_stocks = data.shape\n",
    "\n",
    "  n_train = n_timesteps // 2\n",
    "\n",
    "  train_data = data[:n_train]\n",
    "  test_data = data[n_train:]\n",
    "\n",
    "  env = MultiStockEnv(train_data, initial_investment)\n",
    "  state_size = env.state_dim\n",
    "  action_size = len(env.action_space)\n",
    "  agent = DQNAgent(state_size, action_size)\n",
    "  scaler = get_scaler(env)\n",
    "\n",
    "  # store the final value of the portfolio (end of episode)\n",
    "  portfolio_value = []\n",
    "\n",
    "  if args.mode == 'test':\n",
    "    # then load the previous scaler\n",
    "    with open(f'{models_folder}/scaler.pkl', 'rb') as f:\n",
    "      scaler = pickle.load(f)\n",
    "\n",
    "    # remake the env with test data\n",
    "    env = MultiStockEnv(test_data, initial_investment)\n",
    "\n",
    "    # make sure epsilon is not 1!\n",
    "    # no need to run multiple episodes if epsilon = 0, it's deterministic\n",
    "    agent.epsilon = 0.01\n",
    "\n",
    "    # load trained weights\n",
    "    agent.load(f'{models_folder}/dqn.h5')\n",
    "\n",
    "  # play the game num_episodes times\n",
    "  for e in range(num_episodes):\n",
    "    t0 = datetime.now()\n",
    "    val = play_one_episode(agent, env, args.mode)\n",
    "    dt = datetime.now() - t0\n",
    "    print(f\"episode: {e + 1}/{num_episodes}, episode end value: {val:.2f}, duration: {dt}\")\n",
    "    portfolio_value.append(val) # append episode end portfolio value\n",
    "\n",
    "  # save the weights when we are done\n",
    "  if args.mode == 'train':\n",
    "    # save the DQN\n",
    "    agent.save(f'{models_folder}/dqn.h5')\n",
    "\n",
    "    # save the scaler\n",
    "    with open(f'{models_folder}/scaler.pkl', 'wb') as f:\n",
    "      pickle.dump(scaler, f)\n",
    "\n",
    "\n",
    "  # save portfolio value for each episode\n",
    "  np.save(f'{rewards_folder}/{args.mode}.npy', portfolio_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] -m MODE\n",
      "ipykernel_launcher.py: error: the following arguments are required: -m/--mode\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3-TF2.0]",
   "language": "python",
   "name": "conda-env-py3-TF2.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
